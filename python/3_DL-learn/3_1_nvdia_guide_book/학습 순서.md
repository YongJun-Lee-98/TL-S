---
1~ 4 장: 기본적인  신경망, 3장 이후 부록 A(선형 회귀와 선형 분류기)를 읽어볼 수 있음
5~6장: DL 프레임워크로 시작하기 DL을 가능케 하는 기법들
7장: 합성곱 신경망과 이미지 분류
8장: 잘 알려진 더 깊은 합성곱 신경망 부록 B(탐지와 세분화를 읽어볼 수 있다.
9~11장: 순환 신경망과 시계열 예측
12~13장: 기본적인 단어 임베딩, 부록 C(추가적인 단어 임베딩)를 읽어볼 수 있다.
14~15장: 신경 단어 번역, 어텐션, 트랜스포머, 부록 D(GPT, BERT, RoBERTa)를 읽어볼 수 있다.
16장: 이미지 캡셔닝
17장: 추가적인 주제 모음
18장: 다음 단계
---
포괄적인 트랙
1 ~ 6 장 : 사전 지식에 따라 지나가거나 훑어보기를 권장

컴퓨터 비전 트랙
8장 - 부록  B: 물체 탐지, 의미론적 분할과 객체 분할
9~11장 - 순환신경망과 시계열 예측은 훑어봐도 좋음

언어 처리 트랙
8장 - 잘 알려진 더 깊은 합성곱 신경망은 훑어봐도 좋음
9~11장 - 순환신경망과 시계열 예측
12~13장 - 기본적인 단어 임베딩
부록 C: 추가적인 단어 임베딩
14 ~ 15장 - 신경 단어 번역, 어텐션, 트랜스 포머
부록 D : GPT, BERT, RoBERTa
16장 - 이미지 캡셔닝

## 각 장과 부록의 개요

### 1장 로젠블랫 퍼셉트론
신경망의 기본 토대인 퍼셉트론의 소개
퍼셉트론의 한계에 대해 배우고, 네트워크에 복수의 퍼셉트론을 조합하여 어떻게 한계를 극복하는지 보여준다.
퍼셉트론 및 학습 알고리듬을 구현하는 방법을 보여주는 프로그래밍 예제도 포함함한다.

### 2장 기울기 기반 학습
경사하강 gradient descent 라 알려진 최적화 알고리듬 및 퍼셉트론 학습 알고리듬 이면의  이론을 설명한다. 이는 다층 네트워크를 위한 학습 알고리듬을 설명하는 이후 장의 디딤돌로 쓰임

### 3장 시그모이드 뉴련과 역전파
DNN에서의 자동학습에 쓰이는 역전파 알고리듬을 수학적 용어 그리고 이진 분류를 위해 쓰인 프로그래밍 예제에 모두 통해 설명한다.

### 4장 다중 클래스 분류에 적용된 완전 연결 네트워크
이 장은 데이터셋의 개념과 이들이 어떻게 훈련 집합과 테스트 집합으로 나뉘는지 설명한다. 또한 일반화를 위한 네트워크의 능력을 다룬다. 다중클래스 분류를 다루기 위해 신경망 아키텍처를 확장하고, 프로그래밍 예제 다음 이를 손글씨 숫자를 분류하는 과제에 적용한다. 프로그래밍 예제는 닐슨이 만든 예제에 크게 영향을 받았음

### 5장 DL을 향해: 프레임워크 및 네트워크 미조정
이전 장의 예제를 DL 프레임워크로 구현한다. 
이 프레임워크가 코드를 어떻게 엄청나게 단순화하는지 그리고 네트워크의 여러 변형을 모델링하게 해주는지 보여준다. 또한 더 깊은 네트워크의 여러 변형을 모델링하게 해주는지 보여준다.
더 깊은 네트워크를 훈련시키는데 필요한 많은 기법을 소개함

### 6장 회귀에 적용된 완전 연결 네트워크
이전 장에서 공부한 분류 문제 대신 수치를 예측하는 데 네트워크를 사용하는 방법을 공부함
이는 네트워크를 회귀 문제에 적용하는 프로그래밍 예제로 해보며, 여기서 다수의 변수를 바탕으로 주택의 판매 가격 예측을 시도해본다.

### 7장 이미지 분류에 적용된 합성곱 신경망
합성곱 신경망 혹은 그냥 합성곱 네트워크라 불리는, 2012년에 DL 붐으 ㄹ시작시킨 네트워크의 한 가지 형태를 배운다. CNN은 복수의 문제 도메인에서 쓰일 수 있지만, 이미지 분류/ 분석에 적용할 때 특히 효과적임을 보여왔음. 이것이 어떻게 동작하는지 설명하고 더욱 복잡한이미지 데이터셋을 분류하는 데 CNN을 사용하는 프로그래밍 예제를 검토한다. 이 예제에서는 단지 서로 다른 손글씨 숫자를 구별하는 대신에 비행기, 자동차, 새, 고양이와 같은 더욱 복잡한 물체 클래스를 식별한다.

### 8장 더 깊은 CNN 및 사전 훈련된 모델
GoogLeNet, VGG, ResNet과 같은 더 깊은 CNN을 설명한다. 
프로그래밍 예제로 사전 훈련된 ResNet구현을 다운로드하고 이미지를 분류하는 방법을 배운다.

### 9장 순환신경망으로 시간 시퀀스 예측하기
설명한 네트워크이ㅡ 한계는 입력 길이가 각기 다른 데이터를 다루는 데 잘 맞지 않는다는 점이다. 텍스트와 음성 같은 중요한 문제 도메인은 길이가 달라지는 시퀀스로 구성되는 경우가 많다. 이 장은 이러한 과제를 다루는 데 적합한 순환신경망 아키텍처를 설명한다. 프로그래밍 예제를 사용해 어떻게 이러한 네트워크 아키텍처가 시계열에서의 다음 데이터 포인트를 예측하는 데 쓰이는지 살펴본다.

### 10장 장단기 메모리
RNN이 장기 의존성을 학습하지 못하게 하는 문제를 논의한다. 긴 시퀀스를 더 잘 다루게 해주는 장단기 메모리LSTM 기법을 설명한다.

### LSTM과 빔 검색으로 하는 텍스트 자동완성
장기 예측을 우한 LSTM 기반 RNN을 사용하는 방법을 살펴보고, 빔검색 beam search이라 알려진 개념을 소개한다. 이는 텍스트의 자동완성에 쓰일 수 있는 네트워크 구축하는 프로그래밍 예제로 보여주는데, 자연어 처리 NLP의 더 큰 부분의 부분집합인 자연어 생성NLG,Natural Language Generation의 단순한 예제이다.

### 신경 언어 모델과 단어 임베딩
이전 장의 예제는 단어 대신 개별 글자에 기반하는데 많은 경우 단어 및 그 의미를 가지고 작업하는 것이 개별 글자로 작업하는 것보다 더욱 강력하다. 12장은 개념 언어 모델과 벡터 공간(임베딩 공간)에서의 단어 임베딩을 소개한다. 이는 단어 사이의 중요한 관계를 포착하는 데 쓸 수 있다. 코드 예제로 우리의 자동완성 예제를 글자 대신에 단어로 작업하도록 확장하고 임베딩 공간에서 단어 벡터를 만드는 방법을 살펴본다. 텍스트에서 감정 분석을 할 수 잇는 모델을 구축하는 방법을 논의한다. 이는 NLP의 또 다른 하위 분야인 자연어 이해 예제다.

### word2vec과 GloVe로부터의 단어 임베딩
단어 임베딩을 만들기 위한 두 가지 인기 있는 기법을 논의한다. 기존의 임베딩 집합을 다운로드하고 어떻게 이들이 단어 사이의 다양한 의미론적 관계를 포착하는지 보여준다.

### 시퀀스 투 시퀀스 네트워크와 자연어 번역
이제 두 순환신경망의 조합인 시퀀스 투 시퀀스 네트워크라 알려진 네트워크를 소개하는 장임
이 네트워크의 주요 속성은 출력시퀀스가 입력 시퀀스와 길이가 다를 수 있다는 점
이러한 형태의 네트워크를 이전 장에서 공부한 단어 인코딩과 조합한다. 한 가지 언어로 된 단어 시퀀스를 입력으로 받아서 다른 언어로 출력하는 자연어 번역기를 구현한다. 이때 출력되는 단어의 개수와 순서는 입력과 다를 수 있다. 시퀀스 투 시퀀스 모델은 인코더-디코더 아키텍처라 알려진 아키텍처의 예시다.

### 어텐션과 트랜스포머
인코더-디코더 아키텍처의 정확도를 개선할 수 있는 어텐션이라 부르는 기법을 설명한다. 이를 사용해 어떻게 이전 장의 신경 머신 번역기를 개선할 수 있는지 설명한다. 또한 어텐션 기반 트랜스포머 아키텍처를 설명하는데, 이는 많은 NLP 애플리케이션에서의 핵심 기본 토대다.

### 이미지 캡셔닝을 위한 일대다 네트워크
어떻게 일대다 네트워크를 사용해 이미지 텍스트 설명을 만들고 이러한 네트워크를 어텐션으로 확장하는지 설명한다. 프로그래밍 예제로 이러한 이미지 캡셔닝 네트워크를 구현하고 어떻게 그림 집합의 텍스트 설명을 생성하는데 이를 사용할 수 있는지 보여준다.

### 추가적인 주제 메들리
지금까지 주제가 서로에 기반하여 만들어지도록 구조화했음
17장에서는 이전 장에 포함시킬 좋은 방법을 찾지 못했던 몇 가지 주제를 소개한다. 이러한 주제의 예제로는 오토인코더, 멀티모달 학습, 멀티내스크 학습, 신경 아키텍처 검색 등이다.

### 정리 및 다음 단계
이전 장에서 논의한 주제를 정리하고 요약해 여러분이 이 책에서 설명한 핵심 개념을 잘 이해하고 있는지 확인할 수 있는 기회를 제공한다.
요약에 더해서 여러분이 가고자 하는 방향에 따라 맞춰진, 예를 들면 고도로 이론적이거나 더욱 실제적인 미래의 독서 방향을 제시함
윤리적 AI와 데이터 윤리 관련 주제를 논의함

### 부록 A 선형 회귀와 선형 분류기
기본적인 ML주제를 설명하여 제시된 몇몇 DL 개념이 어떻게 더욱 전통적인 ML기법에 연관되어 있는지 알수 있게한다.

### 부록 B 물체 탐지와 세분화
한 이미지 내의 여러 물체를 탐지하고 분류하는 기법을 설명한다. 이는 또한 물체 주위에 경계 상자를 그리는 거친 기법 및 특정 물체에 해당하는 이미지 내 개별 픽셀을 정확히 집어내는 세밀한 기법 모두를 포함한다.

### 부록 C word2vec과 GloVe 너머의 단어 임베딩
단어 임베딩을 위한 더욱 정교한 기법을 설명한다. 특히 이러한 기법은 훈련 데이터셋에 없는 단어들을 다룰 수 있다. 추가로 단어가 문맥에 따라 의미가 다른 경우를 다룰 수 있는 기법을 설명한다.

### 부록 D GPT, BERT, RoBERTa'
트랜스포머로부터 만들어지는 아키텍처를 설명한다.
이 네트워크 아키텍처는 많은 NLP 과제를 상당히 개선했다.
### 부록 E 뉴턴-랩슨 대 경사 하강
경사 하강이라 부르는 수학적 개념의 기법을 소개하는데, 부록 E는 뉴턴-랩슨이라 알려진 다른 방법 및 이것이 경사 하강과 어떻게 연관이 있는지 설명함

### 부록 F 숫자 분류 네트워크의 행렬 구현
4장에는 파이썬 코드로 신경망을 구현하는 프로그래밍 예제가 포함되는데, 부록 F는 그 프로그래밍 예제 최적화된 변형 두 가지를 설명한다.

### 부록 G 합성곱 층을 수학적 합성곱과 연관시키기
합성곱 신경망을 설명하는데, 이들은 합성곱 이라 알려진 수학적 연산에 근거하며 이로부터 이름을 얻었음
이와 같은 연결을 더 자세히 설명한다.

### 부록 H 게이트 순환 유닛
장단기 메모리라 알려진 네트워크를 설명하는데, 부록 H에서 이 유닛의 더 단순한 버전인 게이트 순환 유닛을 설명한다.

### 부록 I 개발 환경설정

### 부록 J 치트 시트
[다운로드 형태로도 제공됨](http://informit.com/title/9780137470358)
