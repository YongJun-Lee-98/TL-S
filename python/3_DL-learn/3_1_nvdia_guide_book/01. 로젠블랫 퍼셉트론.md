이전 예제
```python
# 퍼셉트론 함수의 파이썬 구현
# 벡터 x의 첫 번째 요소는 반드시 1이여야 한다.
# n개 입력을 가진 뉴런의 w와 x의 길이는 반드시 n+1이여야 한다.

def compute_output(w, x):
	z =0.0
	for i in range(len(w)):
		z += x[i] * w[i] # 가중된 입력의 합을 계산한다.
	if z < 0:
		return -1
	else:
		return 1
```
## 퍼셉트론 학습 알고리듬
이전 예제에서 다소 임의적인 가중치 3개를 골랐으며 이는 입력을 부울값으로 보면 마치 NAND 게이트처럼 움직이는 퍼셉트론이 됐다. 선택한 가중치가 이러한 결과를 내놓은 유일한 것이 아님을 스스로 꽤 쉽게 알 수 있다. 예를 들면, z값은 모든 경우에 0으로부터의 거리가 꽤 멀다는 것을 볼 수 있으므로 가중치 중 하나를 0.1만큼 어느 방향에서든지 바꿔도 여전히 같은 움직임을 볼 수 있어야 한다. 이는 애초에 어떻게 이러한 가중치를 얻게 되었는지 그리고 가중치를 정하는 일반적인 접근 법이 있는지에 대한 의문을 불러온다.
여기서 퍼셉트론 학습 알고리듬이 역할을 한다.
알고리듬 자체를 설명하고 몇 가지 문제에 이를 적용함
이들 실험은 알고리듬이 어떻게 작동하는지에 대한 이해를 제공하지만 또한 퍼셉트론의 한계를 어느정도 드러냄
이러한 한계를 극복하는 것이 가능함을 보여주며 퍼ㅓ셉트론을 다른 각도에서 조사한다.
2장 기울기 기반 하급 에서는 알고리듬이 무엇을 하는지 넘어 좀더 형식적인 추론에 대해 설명한다.

퍼셉트론 학습 알고리듬은 지도 학습 알고리듬이라 부른다.
지도라는 개념은 훈련시키는 모델에 입력 데이터 및 원하는 출력 데이터를 제시함을 뜻함
이 모델이 특정 입력이 해당 출려과 연관이 있음을 학습할 것이라고 기대하면서 선생이 모델에 질문과 답을 제시하는 것으로 생각해보자. 지도 학습의 반대는 비지도 학습으로, 이때 학습 알고리듬은 데이터에서 패턴을 스스로 찾을 책임을 진다. 이러한 개념의 예로는 자연어 텍스트에서 구조를 찾을 수 있는 알고리듬이 있다.
이 개념은 텍스트를 자동완성하는 모델을 훈련시키는 11장 LSTM과 빔 검색으로 하는 텍스트 자동완성에서 더 자세히 알아본다.

> 모델 이란 용어는 네트워크와 같은 뜻을 갖는 경우가 종종 있다.
> 모델 훈련에 관해 이야기할 때 이는 하나 이상의 뉴련으로 구성된 네트워크를 위한 가중치를 찾는 것과 같다.

알고리듬은 다음과 같이 작동한다.
1. 가중치를 무작위로 초기화한다.
2. 입력/출력 쌍 하나를 무작위로 선택한다.
3. 값 x1, ..., xn을 퍼셉트론에 제시하고 출력 y를 계산한다.
4. 출력 y가 입력/출력 쌍의 정답과 다르면, 가중치를 다음과 같은 방법으로 조정한다.
   a. y<0이면, 각 wi에 nxi를 더한다.
   b. y>0이면, 각 wi에 nxi를 뺀다.
5. 퍼셉트론이 모든 예시를 올바르게 예측할 때까지 2, 3, 4 단계를 반복한다.

퍼셉트론은 예측할 수 있는 것에 있어서 특정 한계를 지니므로, 어떠한 입출력 쌍의 집합에서는 알고리듬이 수렴하지 않는다. 그러나 입출력쌍의 집합을 나타낼 수 있게 하는 가중치 집합을 찾을 수 있다면, 이러한 가중치를 찾음으로써 알고리듬이 수렴함을 보장할 수 있다. 임의의 상수 n은 학습률 learning rate3 이라 알려져 있으며 이는 1로 둘 수 있지만, 알고리듬이 더 빨리 수렴하도록 다른 값으로 둘 수도 있다. 학습률은 학습 알고리듬에 의해 조정되지는 않지만 여전히 조정 가능한 매개변수인 초매개변수 hyperparameter의 예시중 하나임
퍼셉트론에서 가중치는 0으로 초기화할 수 있지만, 더 복잡한 신경망에서그렇게 하는 것은 좋은 생각이 아니다. 따라서 이에 익숙해지도록 무작위로 초기화한다.
마지막 4단계에서 모든 가중치가 같은 양으로 조정되는 것처럼 보이지만, 입력 xi가 -1 그리고 1 두 값만을 취하도록 제한되는 것은 아님을 기억하라. 어떤 입력은 0.4 그리고 또 다른 입력은 0.9 일수 잇으므로, 실제 가중치 조정도 달라짐